\chapter{Methods}
\label{chapter:methods}

  The crux of this thesis is the idea that a page request can be generated by
  one of several sources. This section presents the derivations for the
  equations required for this approach. It also discusses some of the design
  decisions and approximations used to translate the theoretical results into
  the working algorithm presented in the supplemental code \cite{supplimental}.

\section{Summary of notation}
  Presented here is a summary of notation that will be used in this section.

  \begin{itemize}
  \item $N$: Number of page requests.

  \item $n$: Index to specify a particular page request.

  \item $R$: Number of page requests recorded in a trace.

  \item $r$: Index to specify a particular element of the trace.

  \item $D$: Number of source distributions.

  \item $d$: Index to specify a particular distribution.

  \item $i$, $j$: Indexes to specify an arbitrary distribution.

  \item $X$: A specific page request.

  \item $x$: An arbitrary page.

  \item 1: Starting value of all indexes.

  \item $\tau_d$: Multinomial probability weight of any given source
  distribution. That is, $\sum_{d=1}^{D} \tau_d = 1$.

  \item $\bm{\theta}_d$: A vector of model parameters for the $d$th
  distribution.

  \item Bold font (e.g. $\bm{\tau}$): A vector of variables. That is,
  $\bm{\tau} = (\tau_1, \tau_2, ...)^{\intercal}$. However, if enough indexes
  are available to refer to a single entry in a vector, I will no longer use
  bold font.

  \item $\bm{Z}_n$: A $1 \times D$ unit vector that has a $1$ in location $d$
  if distribution $d$ was the source for page request $X_n$; otherwise
  $Z_{n,d}$ has the value 0.

  \item $\bm{Z}$: Assignment vector for a page request $X$. $Z_d$ is the
  indicator that identifies whether $X$ came from distribution $d$. $\bm{Z}_n$
  is the assignment vector for a specific $X_n$, while $Z_{d,n}$ indicates
  whether a specific $X_n$ came from distribution $d$.

  \item $\hat{\bm{Z}}$: A stochastic vector that is an approximation of
  $\bm{Z}$. While the entries of $\hat{\bm{Z}}$ are not restricted to the set
  $\{0, 1\}$, the values satisfy $\hat{Z}_d \ge 0$ and $\sum_{d=1}^{D}
  \hat{Z}_{d} = 1$.

  \item $H_d : X \to y$: A function that takes a page request $X$ and produce a
  measurement $y$. The notation $H_d(X)$ is used for these functions. For
  example, the function $H$ could map all page requests $X$ to the number of
  unique pages seen since page $X$ was last requested. In another example, the
  function range of $H$ could be the categories $\{\mbox{read}, \mbox{write}\}$.

  \item $\left< X_n \right>$: The trace sequence of page requests.

  \item $\left< \bm{Z}_n \right>$: The trace sequence of source distribution for
  each page request.

  \item $\left< X_n, \bm{Z}_n \right>$: The trace sequence of page requests and
  assignments.

  \item $C : x \to \mathbb{R^{+}}$: A function, notated with $C(x)$, that
  identifies the cost of fetching page $x$ from the backing store.

  \item $K_n$: The set of all pages $x$ that are resident in cache after the
  $n$th page request.

  \item $|K_n|$: Number of pages held in cache after the $n$th page request.

  \item $|K|$: The size of the cache.

  \item $V_n$: The page evicted after page request $n$, if a page is evicted.

  \item Accent $\hat{}$ (e.g. $\hat{\bm{Z}}$): An estimate.

  \item Accent $\bar{}$ (e.g. $\bar{Z}_d$): An arithmetic average.

  \end{itemize}

\section{The model}
  Assume that a page request $X$ for a page $x$ can come from one of $D$ source
  distributions and that these sources are independent of each other.  If $f$
  represents a probability mass function, then

  \begin{equation}
  \label{eq:mixturemodel}
    \Pr(X = x) = \sum_{d=1}^{D} \tau_d f_d(x | \bm{\theta}_d)
  \end{equation}

  \noindent where $\tau_d$ represents the probability that page $X$ is drawn
  from the $d$th distribution, and $\bm{\theta}_d$ is the collection of model
  parameters for $f_d$. The values $\tau_d$ are weights from a multinomial
  distribution with $\tau_d \ge 0$ and $\sum_{d=1}^{D} \tau_d = 1$.

  For any page request $X$, let $\bm{Z}$ be a $1 \times D$ vector that
  contains a $1$ at location $d$ if $X$ was drawn from the $d$th distribution and
  a $0$ everywhere else.

  Let $K_n$ be the set of pages in cache after the $n$th page request and let
  the function $C(x)$ represent the cost of fetching page $x$ from a backing
  store. If the model parameters $\bm{\tau}$ and $\bm{\theta}$ are known,
  then if a page must be evicted from the cache, the optimal choice $V_n$ is

  \begin{equation}
  \label{eq:optimalreplacement}
    V_n = \argmin_{x \in K_n} C(x) \Pr(X = x) .
  \end{equation}

\subsection{An illustrative example}
  Suppose that we have a cache $K$ that can hold four pages. Now, suppose that
  at some time $n$, we have $K_n = \{42, 1234567891, 1390, 4292014\}$. These
  four values represent unique logical addresses. Now assume a
  two-source mixture distribution with two measurement functions, $H_1 : x
  \to {0, 1, 2, 3}$ and $H_2 : x \to {0, 1, 2, 3}$. The function $H_1$ is the
  measurement of a page's location in the SDD; that is, it identifies how many
  unique pages have been seen since the page $x$ was last seen. $H_2$ is the
  page's frequency rank. If $H_2(1234567891) = 0$ then no pages have been
  requested more frequently than page $1234567891$.

  Now, let the probability mass functions be as follows:

  \begin{equation}
    f_1(x) =
     \begin{cases}
      \frac{8}{15} & \text{if } H_1(x) = 0\\
      \frac{4}{15} & \text{if } H_1(x) = 1\\
      \frac{2}{15} & \text{if } H_1(x) = 2\\
      \frac{1}{15} & \text{if } H_1(x) = 3\\
     \end{cases}
  \qquad
  \mbox{ and }
  \qquad
    f_2(x) =
     \begin{cases}
      \frac{1}{2} & \text{if } H_2(x) = 0\\
      \frac{1}{3} & \text{if } H_2(x) = 1\\
      \frac{1}{6} & \text{if } H_2(x) = 2\\
      0           & \text{if } H_2(x) = 3\\
     \end{cases}
  \end{equation}

  Let $\tau_1 = \frac{3}{4}$ and $\tau_2 = \frac{1}{4}$. The values $H(x)$ will
  need to be measured for all pages $x \in K_n$, but table
  \ref{tab:mixture_example} provides one possible set of measurements. Given
  this information, we can make all page eviction decisions. Note that the sum
  of the expected values adds to 1. This does not eliminate the possibility of
  a cache miss; however, if a cache miss does occur, it means that the model is
  inaccurate.

  \begin{table}[!htbp]
  \begin{tabular}{ | l | l | l | l | l | l |}
    \hline
    $x$ & $H_1(x)$ & $H_2(x)$ & $f_1(x)$ & $f_2(x)$ &
        $\tau_1 f_1(x) + \tau_2 f_2(x)$ \\ \hline
    42         & 0 & 3 & $\frac{8}{15}$ & $0$ &
        $\frac{3}{4} \times \frac{8}{15} + \frac{1}{4} \times 0 =
        0.4$
        \\ \hline
    1234567891 & 1 & 1 & $\frac{4}{15}$ & $\frac{1}{3}$ &
        $\frac{3}{4} \times \frac{4}{15} + \frac{1}{4} \times \frac{1}{3} =
        0.283$
        \\ \hline
    1390       & 2 & 2 & $\frac{2}{15}$ & $\frac{1}{6}$ &
        $\frac{3}{4} \times \frac{2}{15} + \frac{1}{4} \times \frac{1}{6} =
        0.142$
        \\ \hline
    4292014    & 3 & 0 & $\frac{1}{15}$ & $\frac{1}{2}$ &
        $\frac{3}{4} \times \frac{1}{15} + \frac{1}{4} \times \frac{1}{2} =
        0.175$
        \\ \hline
  \end{tabular}
  \caption[A small mixture example]{This is a small example of a mixture model.
  If the algorithm needs to evict a page, it will choose page $1390$ since it
  has the smallest expected value. Note that this is neither the oldest nor the
  least frequently accessed page. It is, however, the most logical eviction
  choice since the cost of retrieving the page multiplied by the probability of
  needing the page in the near future is small.}
  \label{tab:mixture_example}
  \end{table}

\section{Identifying the mixture weights}
\label{sec:identifying_the_mixture_weights}
  In order to use the model in equation \ref{eq:mixturemodel}, we need to know
  the mixture parameters $\bm{\tau}$ for all component distributions in our
  mixture distribution. We also need to know the component distributions $f_d$
  and $\bm{\theta}$, the model parameters that describe the shapes for the
  component distributions.

  We prefer to use the maximum likelihood estimates for $\bm{\tau}$ and
  $\bm{\theta}$; however, we need a way to express the likelihood of a single
  event $X$. Recall that the vector $\bm{Z}$ is a unit vector that identifies
  the source distribution for observation $X$.

  \begin{align}
    \label{eq:arithmean}
    L(X, \bm{Z} | \bm{\tau}, \bm{\theta}) &=
        \sum_{d=1}^{D} Z_d \tau_d f_d(X | \bm{\theta}_d) \\
    \label{eq:geommean}
    L(X, \bm{Z} | \bm{\tau}, \bm{\theta}) &=
        \prod_{d=1}^{D} (\tau_d f_d(X | \bm{\theta}_d))^{Z_d}
  \end{align}

  Equation \ref{eq:arithmean} is the weighted arithmetic mean, while equation
  \ref{eq:geommean} is the weighted geometric mean. We will eventually use
  the estimate $\hat{\bm{Z}} \approx \bm{Z}$ where the entries in the $1
  \times D$ stochastic vector $\hat{\bm{Z}}$ still sum to $1$, but where the
  individual entries are no longer constrained to the set $\{0, 1\}$; thus,
  there is a practical difference between equations \ref{eq:arithmean} and
  \ref{eq:geommean}.

  Since equations \ref{eq:arithmean} and \ref{eq:geommean} are equivalent for
  any unit vector $\bm{Z}$, we are free to choose either, but this choice
  dictates the technique used to estimate $\hat{\bm{Z}}$.  I have chosen
  equation \ref{eq:geommean}, as it is easier to work with the log-likelihood of
  that expression.

  The full likelihood is

  \begin{equation}
  \label{eq:fulllikelihood}
    L \left( \left< X_n, \bm{Z}_n \right> |
             \bm{\tau}, \bm{\theta} \right) =
      \prod_{n=1}^{N} \prod_{d=1}^{D} (\tau_d f_d(X_n |
                                       \bm{\theta}_d))^{Z_{d,n}}
  \end{equation}

  and the log-likelihood is

  \begin{equation}
  \label{eq:loglikelihood}
    \log L \left( \left< X_n, \bm{Z}_n \right> |
                 \bm{\tau}, \bm{\theta} \right) =
        \sum_{n=1}^{N} \sum_{d=1}^{D}
          Z_{d,n} \left( \log(\tau_d) + \log(f_d(X_n | \bm{\theta}_d)) \right) .
  \end{equation}

  In section \ref{sec:using_em}, I outline an approach to identify the model
  parameters. In section \ref{sec:examples}, I provide details for 2 different
  mixture models.

\section{Using the EM algorithm}
\label{sec:using_em}
  One approach estimates the model parameters $\bm{\tau}$ and $\bm{\theta}$ via
  the EM algorithm introduced by Dempster et. al.
  \cite{dempster1977maximum}. This algorithm uses neutral starting estimates
  $\bm{\tau}^{(0)} \approx \bm{\tau}$ and $\bm{\theta}^{(0)} \approx
  \bm{\theta}$. Using the estimates from step $(t)$, we can identify the model
  assignments $\left< \hat{\bm{Z}}_n^{(t)} \right>$ for the observations $\left<
  X_n \right>$. We then use these model assignments to estimate $\bm{\tau}^{(t +
  1)} \approx \bm{\tau}$ and $\bm{\theta}^{(t + 1)}
  \approx \bm{\theta}$.

  This defines an iterative process where the likelihood $L(\left< X_n, \bm{Z}_n
  \right> | \bm{\tau}^{(t)}, \bm{\theta}^{(t)})$ converges to a local maximum
  \cite{wu1983convergence}. If the starting estimates are appropriately chosen,
  the local maximum is also the global maximum and the estimates are arbitrarily
  close to the maximum likelihood estimates.

  While this is a potent algorithm, it does have a few drawbacks:

  \begin{enumerate}
    \item
    \label{enum:em_drawbacks1}
    The EM algorithm requires access to the trace $\left< X_n \right>$. Since a
    computer can run indeterminately, the value $N$ might be impractically
    large. Since it is not practical to maintain the full trace, we need to
    maintain the rolling history $\left< X_r \right>$, where the length of the
    sequence $\left< X_r \right>$ is $R$ and where $R < N$. This introduces the
    tuning parameter $R$. The setting for this tuning parameter affects the
    robustness of the model parameter estimates and how quickly the algorithm
    adapts to changes in the model parameters.

    \item
    \label{enum:em_drawbacks2}
    While the EM algorithm typically converges within a few iterations, the size
    of the trace $R$ is typically some multiple of the cache size $|K|$; thus,
    it is too expensive to run the EM algorithm every time we need decide which
    page $V$ to will evict.

    \item
    \label{enum:em_drawbacks3}
    Choosing good values for $\bm{\tau}^{(0)}$ and $\bm{\theta}^{(0)}$ can be
    difficult \cite{karlis2003choosing}. Good selections cause the EM algorithm
    to converge more quickly, while poor selections cause the algorithm to
    converge to degenerate local optima.
  \end{enumerate}

  One mitigation of the issue presented in item \ref{enum:em_drawbacks2} is to
  run the EM algorithm only periodically. The frequency of EM algorithm runs is
  another tuning parameter. If model parameters are updated too frequently, the
  updates dominate the cost of page eviction; however, too-infrequent updates
  adapt too slowly to changes in the model parameters.

  One control is to run the EM algorithm after $\log(R)$ page requests. The
  expression $R|K|$ can be used to indicate that the value $R$ is a function of
  the cache size $|K|$. Big O notation can be used to identify the cost of using
  the EM algorithm:

  \begin{align*}
    EM(\left< X_r \right>)
      &= O(\log(r(|K|)) \\
      &= O(\log(|K| * c) \\
      &= O(\log(|K|)
  \end{align*}

  A reasonable approach to address the issue in item
  \ref{enum:em_drawbacks3} is to start the EM algorithm by assigning $Z_{d, r}
  = \frac{1}{D}$, where $D$ represents the number of mixtures in the model. This
  has the interesting effect of making the initial estimates for each
  $\bm{\theta}_d$ take the same values as if all items in the trace
  $\left< X_r \right>$ came from the $d$th distribution.

\subsection{Using a rolling expectation algorithm}
\label{sec:using_rolling}
  Rather than treat our estimated model parameters as constants between
  invocations of the EM algorithm, we can instead use an incremental algorithm to
  modify model parameters \cite{neal1998view}. Since we can only maintain a
  limited trace of $r$ entries, whenever we process a page request, we can
  account for the effects of adding the new item and removing the old item.

  An example of this algorithm is presented in section
  \ref{sec:example_mix_lru_lfu}.

\section{Examples}
\label{sec:examples}
  These examples are selected to demonstrate the versatility of the MMC
  approach.

\subsection{A mixture of LRU and LFU policies}
\label{sec:example_mix_lru_lfu}
  Many existing caching algorithms approximate some combination of recency
  and frequency measurements \cite{arc, johnson1994x3, kim2001lrfu, o1993lru}.
  However, rather than use heuristics to make decisions based on these
  measurements, we can use equation \ref{eq:mixturemodel} to specify a general
  mixture model for the page request behavior, and based on recency and
  frequency measurements, we can use the EM algorithm to pick optimal model
  parameters.

  We first need to choose the distributions with probability mass functions
  $f_d$. I arbitrarily choose to use the geometric distribution $\Geom$ with

  \begin{equation}
    H(X) | \theta \sim \Geom(\theta)
  \qquad
  \mbox{and}
  \qquad
    \Pr(H(X) = y) = \theta (1 - \theta)^{y}, y \in \{0\} \cup \mathbb{R}^{+}
  \end{equation}

  \noindent to model both the stack depth distribution (SDD) and the independent
  reference model (IRM) distribution. Aside from the simplicity of the math, I
  am not aware of a compelling reason that a mixture of geometric distributions
  serves as a better model than, say, a mixture of a Poisson distribution for
  the SDD and a negative binomial distribution for the IRM.

  Some potential problems exist when trying to use the geometric distribution
  $\Geom$ to model the IRM. If items are sorted with the elements of highest
  probability first, then the geometric distribution specifies the probability
  that an arbitrary page $x$ will be found at the $H(x)$th location in the list.
  However, under the IRM, how should we order the pages in virtual memory so
  that we can talk about the first, second, and so on? The approach I use is to
  rank the pages based on the estimated number of IRM requests seen in the last
  $r$ requests.

  If we let $H_d(x)$ be a function that identifies the location of element $x$
  in the $d$th distribution, we can write the probability mass function as

  \begin{align}
  \label{eq:mixpdf}
    \Pr(X = x) &=
        \sum_{d = 1}^{2} \tau_d \theta_d (1 - \theta_d)^{H_d(x)} .
  \end{align}

  It does not matter to this derivation if we decide that the SDD will be
  distribution $d = 1$ or if we decide that it will be distribution $d = 2$.
  However, in order to clarify the discussion, I will use the convention that
  $d = 1$ refers to the stack depth distribution (SDD) while $d = 2$ refers to
  the independent reference model (IRM).

  In the expectation (E) step of the EM algorithm, we assign each page request
  $X$ to a distribution $d$ using
  $\hat{Z}_{d, n}^{(t)} = E(Z_{d, n} | \bm{\tau}, \bm{\theta})$. We
  treat the values $\bm{\tau}^{(t)}$ and $\bm{\theta}^{(t)}$ as known constants.
  To begin the derivation, we can use Bayes theorem to express the expected
  value of $Z_{i, r}$ (in this case, $i \in \{1, 2\}$):

  \begin{align}
    \hat{Z}_{i, r} &= E(Z_{i, r}) \\
      &= \Pr(Z_{i, r} = 1 | X_r) \\
  \label{eq:estepforz}
      &= \frac{\Pr(X_r | Z_{i, r} = 1) \Pr(Z_{i, r} = 1)}{\Pr(X_r)} .
  \end{align}

  All of these factors are known:

  \begin{align}
    \Pr(Z_{i, r} = 1) &= \tau_i^{(t)} \\
    \Pr(X_r | Z_{i, r} = 1) &= \theta_i^{(t)} (1 - \theta_i^{(t)})^{H_i(X_r)} \\
    \Pr(X_r) &=
        \sum_{d = 1}^{2}
        \tau_d^{(t)} \theta_d^{(t)} (1 - \theta_d^{(t)})^{H_d(X_r)}
  \end{align}

  Rewriting \ref{eq:estepforz}, we have

  \begin{equation}
  \label{eq:Z_estimate}
    \hat{Z}_{i, r}^{(t)} =
        \frac{\tau_i^{(t)} \theta_i^{(t)} (1 - \theta_i^{(t)})^{H_i(X_r)}}
        {\sum_{d = 1}^{2}
         \tau_d^{(t)} \theta_d^{(t)} (1 - \theta_d^{(t)})^{H_d(X_r)}} .
  \end{equation}

  For the maximization (M) step, we can use the log-likelihood of our trace
  $\left< X_r \right>$, together with the estimates of our missing values
  $\left< \hat{\bm{Z}}_r^{(t)} \right>$, to find the maximum likelihood
  estimates (mle) for our model parameters. To begin, we need an expression for
  the likelihood of $\left< X_r, \hat{\bm{Z}}_r \right>$. Since we will allow
  the $\hat{Z}_r$ values to take on fractional values, we need a way to weight the
  observations from the two component distributions. However, the arithmetic
  mean produces an awkward log-likelihood; therefore, we will use the geometric
  mean, as discussed in section \ref{sec:identifying_the_mixture_weights}.

  The likelihood for a single observed pair $(X_r, \bm{Z}_r)$ is

  \begin{align}
    L\left( (X_r, \bm{Z}_r ) | \bm{\tau}, \bm{\theta} \right) =
        (\tau_1 f_1(X_r | \theta_1))^{Z_{1, r}^{(t)}}
        (\tau_2 f_2(X_r | \theta_2))^{Z_{2, r}^{(t)}} .
  \end{align}

  The log-likelihood for the sequence $\left< X_r, \bm{Z}_r \right>$ is derived as

  \begin{align}
    L \left( \left< X_r, \bm{Z}_r \right> | \bm{\tau}, \bm{\theta} \right) &=
      \prod_{r = 1}^{R}
        (\tau_1 f_1(X_r | \theta_1))^{Z_{r, 1}^{(t)}}
        (\tau_2 f_2(X_r | \theta_2))^{Z_{r, 2}^{(t)}} \\
    \log \left( L \left( \left< X_r, \bm{Z}_r \right> | \bm{\tau}, \bm{\theta}
    \right)\right) &=
      \sum_{r=1}^{R} \sum_{d=1}^{2}
        Z_{d, r}^{(t)} (\log(\tau_d) + \log(f_d(X_r | \theta_d))) \\
    &=
      \sum_{r = 1}^{R} \sum_{d=1}^{2}
        Z_{d, r}^{(t)} (\log(\tau_d) + \log(\theta_d) + H_d(X_r) \log(1 -
        \theta_d)) .
  \end{align}

  To find the mle for $\tau_i$, we take

  \begin{align}
    0 &= \frac{d}{d \tau_i} \log(L(\left< X_r, \bm{Z}_r \right> |
                                   \bm{\tau}, \bm{\theta})) \\
    0 &=
        \sum_{r=1}^{R}
          \left(
            \frac{Z_{i, r}^{(t)}}{\hat{\tau_i}} -
            \frac{1 - Z_{i, r}^{(t)}}{1 - \hat{\tau_i}}
          \right) \\
    \label{eq:tau_i_pre_simplification}
    \hat{\tau}_i &= \frac{\sum_{r=1}^{R} Z_{i, r}^{(t)}}{R} \\
    \hat{\tau}_1 &= \frac{\sum_{r=1}^{R} Z_{1, r}^{(t)}}{R} \\
    \hat{\tau}_2 &= \frac{\sum_{r=1}^{R} Z_{2, r}^{(t)}}{R} = 1 - \hat{\tau}_1
  \end{align}

  To find the mle for $\theta_i$, we follow a similar process:

  \begin{align}
    0 &= \frac{d}{d \theta_i} \log(L(\left< X_r, \bm{Z}_r \right> |
                                     \bm{\tau}, \bm{\theta})) \\
    0 &=
      \sum_{r=1}^{R}
        \left(
          \frac{Z_{i, r}^{(t)}}{\theta_i} -
          \frac{Z_{i, r}^{(t)} H_i(X_r)}{1 - \theta_i}
        \right) \\
    \theta_i \sum_{r=1}^{R} Z_{i, r}^{(t)} H_i(X_r) &=
      (1 - \theta_i) \sum_{r=1}^{R} Z_{i, r}^{(t)} \\
    \theta_i
      \sum_{r=1}^{R}
        \left( Z_{i, r}^{(t)} H_i(X_r) + Z_{i, r}^{(t)}
        \right) &= \sum_{r=1}^{R} Z_{i, r}^{(t)} \\
    \label{eq:theta_i_pre_simplification}
    \theta_i &=
      \frac{\sum_{r=1}^{R} Z_{i, r}^{(t)}}
           {\sum_{r=1}^{R} \left(
              Z_{i, r}^{(t)} H_i(X_r) + Z_{i, r}^{(t)} \right)} \\
    \label{eq:theta_i_post_simplification}
    \theta_i &= \frac{R \tau_i}{R \tau_i + \sum_{r=1}^{R} Z_{i, r} H_i(X_r)}
  \end{align}

  A convenient first step in our algorithm is to arbitrarily assign $Z_{d,
  r}^{(0)} = \frac{1}{2}$ for all $d$ and $r$. We will follow this with an M
  step, and afterward we will alternate between the M step and the E step an
  arbitrary number of times. Since our final goal is to identify the model
  parameters $\bm{\tau}$ and $\bm{\theta}$, we will end after an E step.

  If $\tau_1 = 1$, the model reduces to a least recently used (LRU)
  algorithm. If $\tau_2 = 1$, the model reduces to a variation of the least
  frequently used (LFU) algorithm where frequency counts are \lq\lq forgotten" when
  elements fall off the trace.

\subsection{Accounting for read/write information}
\label{sec:accounting_for_rw}
  We can make several qualitative observations about a page request $X$.
  Three qualitative observations are of particular interest:

  \begin{enumerate}
  \item
  \label{item:qualitative_1}
  When a process requests a page, it can either read the page, or it can modify
  the page. When the caching system responds to a page request, it can detect
  whether the page is being read or written.

  \item
  \label{item:qualitative_2}
  On a cache miss the system can observe whether the address of the requested
  page is a very short distance from a page that was resident in cache. The
  idea here is that device drivers tend to arrange files linearly in physical
  memory. If we are scanning a file, then we will frequently read a page that
  has an address that is within one address of a recently read page, after
  which we will typically not reference the page again.

  \item
  \label{item:qualitative_3}
  We can record whether a page has been referenced at least once since it was
  brought into cache. This qualitative observation is motivated by the mechanism
  used in the 2Q and ARC algorithms to move pages to a second eviction queue
  \cite{arc, johnson1994x3}.
  \end{enumerate}

  Equation \ref{eq:mixturemodel} is versatile enough to allow an algorithm to
  utilize any of these measurements. However, item \ref{item:qualitative_1}
  does not suggest a source distribution for a page request, whereas items
  \ref{item:qualitative_2} and \ref{item:qualitative_3} do. A method that will
  deal with this situation is to use twice as many source distributions to
  account for whether a page request is a read or a write, whereas the other
  categorical variables can be used to define a single additional source
  distribution.

  This section describes how an algorithm can give special consideration to whether
  the last request for a page $x$ was a read or a write. Let us use the function
  $\Read : x \to {\mbox{read}, \mbox{write}}$ to indicate whether the last
  request for page $x$ was a $\mbox{read}$ or a $\mbox{write}$. Unfortunately,
  for an arbitrary $X = x$, we cannot identify whether the last request to the
  page $x$ was a read or a write. We can only identify this information with
  certainty if the page request was a cache hit.

  Let us once again use a geometric distribution to describe each of our source
  distributions. Our mixture model will then be

  \begin{align}
    \Pr(X = x) &= \sum_{d=1}^{4} \tau_d \Geom(H_d(x) | \theta_d) \\
               &= \sum_{d=1}^{4} \tau_d \theta_d (1 - \theta_d)^{H_d(x)} .
  \end{align}

  The variable $\bm{Z}$ is now a $1 \times 4$ unit vector. However, since we
  have additional data available whenever we have a cache hit, the estimate
  $\hat{\bm{Z}} \approx \bm{Z}$ will have a $0$ in two of its entries.

  Consider the following distribution assignments:
  \begin{itemize}
    \item
    $d = 1$ represents that a read page request came from the stack depth
    distribution (SDD).

    \item
    $d = 2$ represents that a read page request came from the independent
    reference model (IRM).

    \item
    $d = 3$ represents that a write page request came from the SDD.

    \item
    $d = 4$ represents that a write page request came from the IRM.
  \end{itemize}

  In order to derive the estimate $\hat{\bm{Z}} \approx \bm{Z}$, we need to deal
  with two cases; the first case is when the page request $X$ is a cache hit and
  the second case is when the page request $X$ is a cache miss.

  Let us first deal with the case when $X$ is a cache miss. Here, our estimate
  is nearly identical to the estimate listed in equation \ref{eq:Z_estimate};
  the only difference now is that we have $D = 4$ instead of $D = 2$:

  \begin{equation}
    \hat{Z}_{i, r}^{(t)} =
        \frac{\tau_i^{(t)} \theta_i^{(t)} (1 - \theta_i^{(t)})^{H_i(X_r)}}
        {\sum_{d = 1}^{4}
         \tau_d^{(t)} \theta_d^{(t)} (1 - \theta_d^{(t)})^{H_d(X_r)}} .
  \end{equation}

  This requires us to derive the estimate $\hat{\bm{Z}} | \Read(x) \approx
  \bm{Z}$. In order to simplify the notation, let us use the convention that $i$
  and $j$ represent source distributions with $i \ne j$ and $i, j \in \{1, 2\}$ or
  $i, j \in \{3, 4\}$. For example, we have $i = 1 \implies j = 2$ and $i = 4
  \implies j = 3$. From this, we have

  \begin{align}
    \hat{Z}_{i, r}
      &= E(Z_{i, r} | \Read(x)) \\
      \label{eq:Z_general}
      &= \frac
        {\Pr(X_r | Z_{i, r} = 1) \Pr(Z_{i, r} = 1 | \Read(x)}
        {Pr(X_r | \Read(x))} .
  \end{align}

  Each of the factors are also known:

  \begin{align}
    \Pr(Z_{i, r} = 1 | \Read(x)) &=
      \begin{cases}
        0 \mbox{ if } \Read(x) = 0 \mbox{ and } i \in \{1, 2\} \\
        0 \mbox{ if } \Read(x) = 1 \mbox{ and } i \in \{3, 4\} \\
        \frac{\tau_i}{\tau_i + \tau_j} \mbox{ otherwise}
      \end{cases} \\
    \Pr(X_r | Z_{i, r} = 1) &= \theta_i (1 - \theta_i)^{H_i(X_r)} \\
    \Pr(X_r | \Read(x)) &=
      \frac{\tau_i}{\tau_i + \tau_j} \theta_i (1 - \theta_i)^{H_i(X_r)} +
      \frac{\tau_j}{\tau_i + \tau_j} \theta_j (1 - \theta_j)^{H_j(X_r)}
  \end{align}

  Using this to rewrite equation \ref{eq:Z_general}, we have

  \begin{equation}
    \hat{Z}_{i, r} =
      \begin{cases}
        0 \mbox{ if } \Read(x) = 0 \mbox{ and } i \in \{1, 2\} \\
        0 \mbox{ if } \Read(x) = 1 \mbox{ and } i \in \{3, 4\} \\
        \frac
          {\tau_i \theta_i (1 - \theta_i)^{H_i(X_r)}}
          {\tau_i \theta_i (1 - \theta_i)^{H_i(X_r)} +
           \tau_j \theta_j (1 - \theta_j)^{H_j(X_r)}
          } \mbox{ otherwise.}
      \end{cases}
  \end{equation}

  Now we need to find estimates for $\bm{\tau}$ and $\bm{\theta}$. The
  likelihood and log-likelihood equations are:

  \begin{equation}
  \label{eq:fulllikelihood_rw}
    L \left( \left< X_r, \bm{Z}_r \right> |
             \bm{\tau}, \bm{\theta} \right) =
      \prod_{r=1}^{R} \prod_{d=1}^{4} \left(
        \tau_d \theta_d (1 - \theta_d)^{H_d(X_r)} \right)^{Z_{d, r}}
  \end{equation}

  \noindent
  and the log-likelihood is

  \begin{equation}
  \label{eq:loglikelihood_rw}
    \log L \left( \left< X_r, \bm{Z}_r \right> |
                 \bm{\tau}, \bm{\theta} \right) =
        \sum_{r=1}^{R} \sum_{d=1}^{4}
          Z_{d,r} \left(
            \log(\tau_d) +
            \log(\theta_d) +
            H_d(X_r) \log(1 - \theta_d)
          \right)
  \end{equation}

  With respect to $\bm{\tau}$, equation \ref{eq:fulllikelihood_rw} is the
  likelihood of a multinomial distribution. Recognizing this, we can use the
  maximum-likelihood estimate of $\hat{\tau}_i \approx \frac{\sum_{r=1}^{R}
  Z_{i, r}}{R} = \bar{Z}_d$. See \cite{murphybinomial} for a cogent derivation
  of the maximum-likelihood estimate for a multinomial distribution.

  This leaves us needing to solve for $\theta_i$.

  \begin{align}
    0 &= \frac{d}{d \theta_i} \log L \left(
      \left< X_r, \bm{Z}_r \right> | \bm{\tau}, \bm{\theta} \right) \\
      &= \sum_{r=1}^{R}
      \left(
        \frac{Z_{i,r}}{\theta_i} - \frac{Z_{i, r} H_i{X_r}}{1 - \theta_i}
      \right) \\
    \theta_i &= \frac{R \tau_i}{R \tau_i + \sum_{r=1}^{R} Z_{i,r} H_i(X_R)} .
  \end{align}

  The equations needed for the EM algorithm are nearly identical to the
  equations in section \ref{sec:example_mix_lru_lfu}. The primary difference is
  in how the assignment vector $\hat{\bm{Z}}$ is computed.

\subsection{Significant implementation choices}
  An implementation of the algorithms described in sections
  \ref{sec:example_mix_lru_lfu} and \ref{sec:accounting_for_rw} is available in
  the supplementary software \cite{supplimental}. I used several approximations
  in this implementation. In this section, I discuss some of the more important
  ones.

  First, I decided to maintain a partitioning of the cache. In all of the
  simulations found in chapter \ref{chapter:results}, I split the cache $K$ into
  two equal and disjoint subsets. One subset, $k \subset K$, represents all
  pages that are actively being stored. The other subset, $K - k$, represents
  pages that have recently been evicted; however, we are still maintaining
  metadata about these pages. I chose to set the sizes $2|k| = |K|$ to
  facilitate a better comparison with the ARC algorithm \cite{arc}.

  I chose to run the EM algorithm periodically after $50 \times \log(R)$ steps.
  However, I also implemented a rolling update scheme for the model parameters
  $\bm{\tau}$ and $\bm{\theta}$. I observed that equations
  \ref{eq:tau_i_pre_simplification} and \ref{eq:theta_i_post_simplification}
  both use an accumulator. Starting with equation
  \ref{eq:tau_i_pre_simplification}, I used the following equation to update the
  estimate:

  \begin{align}
  \label{eq:roll_tau}
    \hat{\tau}_i &=
      \frac{\sum_{r=1}^{R} \left(Z_{i, r}^{(t)} \right) -
            Z_{i, 1} + Z_{i, R + 1}}
           {R} .
  \end{align}

  I updated equation \ref{eq:theta_i_post_simplification} using

  \begin{align}
  \label{eq:roll_theta}
    \theta_i &=
      \frac
        {R \tau_i}
        {R \tau_i + \sum_{r=1}^{R} \left( Z_{i, r} H_i(X_r) \right) -
          Z_{i, 1} H_i(X_1) + Z_{i, R + 1} H_i(X_R)
        } .
  \end{align}

  Neither of these updates is perfect since the EM algorithm requires that we
  update all of the $\hat{\bm{Z}}$ values in response to an update of the model
  parameters. The approximation in equations \ref{eq:roll_tau} and
  \ref{eq:roll_theta} both bypass the E step.

  Some corner-cases concerning the functions $H(X)$ need to be addressed.

  In the SDD, $H_1(x)$ represents the depth of page $x$. However, we will
  frequently encounter a cache miss where we have not previously encountered a
  request for page $x$. I experimented with 4 solutions to this issue:

  \begin{enumerate}
  \item
  \label{list:cache_miss_solution_1}
  Modify equation \ref{eq:theta_i_pre_simplification} to only account for
  the values $r$ where $X_r$ was a cache hit. This produced a stable and
  workable algorithm, but the final value of $\theta_1$ appeared to be biased in
  favor of larger values.

  \item
  \label{list:cache_miss_solution_2}
  Set $H_1(x | \mbox{miss}) = |K| + \frac{1}{\theta_1}$. This choice is
  motivated by the fact that the geometric distribution is memoryless. However,
  this produced an unstable algorithm where $\theta_1$, and hence $\bm{\tau}$
  and $\hat{\bm{Z}}$ were unstable. This appeared to be due to a mismatch
  between the two-source mixture model and reality. In a real-world trace, we
  expect to see a large number of cache misses as the working set of processes
  change. The mixture model in this example requires that the cache miss be
  assigned to one of the two source distributions.

  \item
  Set $H_1(x | \mbox{miss}) = \frac{1}{\theta_1}$. This solution had an effect
  similar to item \ref{list:cache_miss_solution_1}. I used this solution in the
  simulations. The benefit of this solution is that the expected depth of an
  element in the SDD, before we know whether it was a cache miss, is
  $\frac{1}{\theta_1}$.

  \item
  Set $H_1(x| \mbox{miss}) = \tau_1 |K| + \frac{1}{\theta_1}$. The theory here
  is that the number of entries in the SDD is $|K|$ even though only $\tau_1$ of
  those entries were placed there by a SDD request. This was more stable than
  item \ref{list:cache_miss_solution_2}, but it occasionally produced wild
  estimates.

  \end{enumerate}

  Cache misses did not affect the IRM nearly as drastically. Since the IRM
  assumes that $\Pr(X = x)$ is a constant value, after we observe a request
  for page $x$ we can immediately update our estimate for $\Pr(X = x)$. Since we
  cannot record more than $|K|$ pages, the rank for page $x$ is no worse than
  $|K| - 1$.

  However, the IRM did pose problems when a trace element referred to a page that
  had been purged from the cache $K$. An estimate for $H_2(x)$ is required for
  the EM algorithm and for the rolling model parameter updating scheme outlined
  in equation \ref{eq:roll_theta}.

  Within the EM algorithm, whenever a trace element referred to a page $x \not\in
  K_n$, I used the estimate $H_2(x| \mbox{purge}) = \frac{1}{\theta_2}$.
  However, this did not work for the rolling updating scheme in equation
  \ref{eq:roll_theta}. The issue is that the model parameter $\theta_i$ depends
  on an accumulator. When we \lq\lq forget" an element and remove its effects
  from the accumulator, we need to use the value of $H_2(X_1)$ that was
  originally used, not the estimate that we would like to use for an element
  that is not in the cache. In order to address this issue, I augmented the
  trace to record the last estimated value for the rank of a page in the IRM. I
  updated this value whenever the EM algorithm ran.

  Only a few significant modifications were required to account for read/write
  information, as described in \ref{sec:accounting_for_rw}. First, $\bm{\tau}$,
  $\bm{\theta}$, and $\hat{\bm{Z}}$ are all $1 \times 4$ vectors as opposed to
  the $1 \times 2$ vectors that were otherwise required. Every page and trace
  element recorded whether the last request for that page was a read or a write.
  Consequently, the calculation for $\hat{\bm{Z}}$ became a bit more complex in
  order to account for this information.

